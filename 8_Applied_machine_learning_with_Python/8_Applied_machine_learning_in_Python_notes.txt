1 Fundamentals of machine learning - intro to scikit-learn
- supervised machine learning - to predict target values from labelled data:
a. classification (target values are discrete) -> a classifier
b. regression (target values are continuous) -> a regression function
- source of labelled data: Amazon's Mechanical Turk, Crowd Flower
- unsupervised machine learning - find structure in unlabelled data:
a. clustering (find groups of similar instances)
b. outlier detection (find unusual patterns)
- important Python libraries: scikit-learn, pandas, numpy, scipy, matplotlib
- kNN (k Nearest Neighbours) is usually used for supervised machine learning (as a classifier)
- typically, the number of k is odd to ensure the simple majority vote (the more votes for the same category the easier to assign the test query point to the category)
- parameter of kNN:
a. distance (Euclidean, Minkowski)
b. how many NN
c. weighting (simple majority rule)
d. to assign classes to groups
- standard procedure: get data -> clean data -> create training and test samples -> decide on the estimator (classifier/regressor) -> fit the data on training sample (model fitting produces a trained model) -> predict the result using test sample -> evaluate the model (accuracy score)
- training is the process of estimating model parameters
- features (independent variables , usually denoted by X since it is a matrix)
- target variable/lable (dependent variable to predict, usually denoted by y since it is a scalar)

================================================================================

2 Supervised machine learning
- kNN is easy to use and understand but very slow when handling a lot of features
- kNN could represent a baseline or benchmark towards other more sophisticated models
- kNN default number of neighbours: 5
- kNN default metric: distance between data points (Minkowski distance with power parameter = 2. i.e. Euclidean)