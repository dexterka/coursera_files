1 Working with text in Python 
- istitle() is a function that finds the capitalized words
- xyz matches the exact order of characters in a sentence 
- [xyz] matches any character from xyz
- [^xyz] matches none of the characters xyz
- ? matches at most one character
- {1,3} means that a certain character can appear at least once and at most three times

================================================================================
2 Basic Natural Language Processing
- NLP tasks: unique words, sentence boundaries, word counts, parse the sentence structure, identify semantics roles, identify entities in a sentence, finding which noun relates to which entity (co-reference)
- NLTK = natural language toolkit (access to numerous word corpuses via nltk.download())
- normalization of text = if there are different forms of the same word (to lower case and split by white space)
- stemming of the text = to find the root form of the word
- lemmatization of the text = meaningful words, it is stemming but resulting stems are all valid words
- tokenization = full stops and abbreviations are kept as separate words/tokens
- sentence splitting (nltk.word_tokenize() vs. nltk.sent_tokenize())
- POS tagging (part-of-speech) - useful for feature extraction (f.e. extract only verbs, nouns, etc.)

================================================================================
3 Classification of text
- it's about supervised learning
- usual tasks: topic identification (politics/sports/technology), spam detection (spam/not spam), sentiment analysis (positive/negative), spelling collection
- classification algorithms: naive bayes classifiers, support vector machines

a) Naive Bayes classifiers
- based on probabilistic model
- it's about the update of the likelihood of the given class (label) based on new information
- Bayes rule = posterior probability = (prior probability * likelihood) / evidence
- naive assumption: given the class label, features are assumed to be independent of each other
- a word would be assigned to a certain group which has the maximum probability
- if there are no occurrences of a word in a certain class then we do not use 0 but we use Laplace or Additive smoothing, i.e. we add a dummy count (+1) to all the occurences
- Naive Bayes model should be the first to try in text analysis because it is very strong and could give you the baseline measure in performance comparison
- variations of Naive Bayes classifiers:
-- i) multinomial - we are interested in the frequency of each word in the text
-- ii) Bernoulli - we are only interested if the word is present in the text or not

b) Support Vector Machines
- good for highly dimensional data
- handles only numeric features (convert categorical features to numeric ones)
- hard to interpret
- it's about decision boundaries
- linear boundaries are easy to find and evaluate (and do not tend to be overfitting = are more general)
- support vector machines are maximum margin classifiers, they're linear classifiers that find a hyperplane to separate two classes of data = positive and negative
- regularization parameter C (the higher the values the less regularization is put on model = every data point is important eventhough the general performance of the model is low)
- type of decision boundary is another parameter and it is represented by kernel (linear, polynomial, radial basis function, etc.)
- if we have multiple classes (not binary) there are 2 approaches: one vs. the rest of the classes (preferred because of less classifiers to train) and one vs. one
- different classes could have different weights

TF-IDF values for different words is acceptable, and means that those words have the same ratio between their term frequency (TF = measure of occurrences of word in one document) to the inverse document frequency (IDF = measure of frequency of the word across all documents). A higher value represents a higher level of significance.

================================================================================
4 Topic modeling
- WordNet is a dictionary of mostly English words interlinked by semantic relations
- first define the context, then the strength of their association (=how often do they appear together)
- topic modeling is a form of text clustering (exploratory analysis)
- generative models for text (LDA = Latent Dirichlet Allocation)
- separate models for modeling individual topics
- a mixture model is a model that models for a certain number of topics (f.e. 5 topics)
- the number of topics should be known prior to modeling
- gensim, lda Python packages
- pre-processing is necessary prior to modeling:
a) tokenize, normalize(lowercase)
b) stop word removal
c) stemming
- the tokens are then converted to document-term matrix which is further used in LDA models
- the process of text mining is usually based on named entity recognition,  relation extraction and co-reference resolution
- named entity recognition can be divided into:
a) boundary detection
b) tagging/classification
- approaches to named entity recognition:
a) regex (phone numbers, dates, etc.)
b) machine learning approach
- named entity recognition is usually a four-class model identifying:
a) persons
b) organizations
c) locations
d) everything else 
- co-reference resolution - disambiguate mentions in text and group mentions together

